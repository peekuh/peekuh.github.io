<!doctype html><html lang=en><head><title>Optimizing a DRF application for I/O bound, AI workloads :: srijan's blog</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta name=keywords content="django,drf,gevent,gunicorn,async"><meta name=robots content="noodp"><link rel=canonical href=../../posts/drf-optimization/><link rel=stylesheet href=../../styles.css><link rel="shortcut icon" href=../../img/theme-colors/orange.png><link rel=apple-touch-icon href=../../img/theme-colors/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Optimizing a DRF application for I/O bound, AI workloads"><meta property="og:description" content><meta property="og:url" content="/posts/drf-optimization/"><meta property="og:site_name" content="srijan's blog"><meta property="og:image" content="/"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2025-10-02 00:00:00 +0000 UTC"></head><body class=orange><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=../../><div class=logo>srijan's blog</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=../../about>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=../../about>About</a></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=../../posts/drf-optimization/>Optimizing a DRF application for I/O bound, AI workloads</a></h1><div class=post-meta><time class=post-date>2025-10-02</time><span class=post-author>Srijan</span>
<span class=post-reading-time>5 min read (1026 words)</span></div><div class=post-content><div><p>DRF is my favourite toolkit for building Web API’s.</p><p>However, it has one fatal flaw – it does not support async out of the box. This shouldn&rsquo;t be a problem for most use cases but if your application <code>I/O</code> heavy (basically every GPT wrapper on the market right now), this can be a significant bottleneck.</p><h3 id=what-causes-this-bottleneck>What causes this bottleneck?<a href=#what-causes-this-bottleneck class=hanchor arialabel=Anchor>&#8983;</a></h3><p>The <code>GIL</code> prevents more than one thread within a process from executing python bytecode at any given point in time. Which means, while your worker is waiting for a response from an <code>I/O</code> call (api call to Gemini, Anthropic, Openai etc), it just sits there doing nothing. Any other requests assigned to the worker will have to wait until the first request concludes.</p><p>There’re a couple of ways to overcome this bottleneck.</p><p>For starters, you could try increasing the number of workers used by your application server. So when one of your workers is busy waiting for an <code>I/O</code> call, another worker can handle other requests in the queue.
This is the easiest form of horizontal scaling. However, workers are “process bound”.</p><p>A program, (your DRF code) is merely a collection of passive instructions. The active execution of these instructions is called a process.
Processes have an independent memory space, a copy of the python interpreter, a copy of your application’s code, and most importantly their own <code>GIL</code>.</p><p>This means you can only have a finite number of processes running on a CPU at any given time, considering the high resource requirement that processes bring. This finite number is not nearly large enough to handle the scale modern web applications operate at.
The number of workers you can spawn on a server is usually <code>2n + 1</code>, where <code>n</code> is the number of cores on the CPU.</p><p><img src=../../10.png alt="Granian worker memory usage"></p><p>Each <code>pid</code> corresponds to one Granian (an http server similar to <code>gunicorn</code>) worker. Notice how each worker takes up between 400 to 600 mb of memory.</p><p>To put things into perspective, an i9-9900K, a 16 core CPU, would be able to host 33 workers at most.</p><p>But all of these limitations exist assuming we only have one thread per process. What if we had multiple threads capable of handling multiple requests?</p><h2 id=preemptive-multitasking>Preemptive multitasking<a href=#preemptive-multitasking class=hanchor arialabel=Anchor>&#8983;</a></h2><p>This is where <code>gunicorn</code>’s <code>gthread</code> worker type comes in.
While thread A waits for a response from a network call, it surrenders the <code>GIL</code> to thread B. This means thread B is now free to handle other requests. Once thread A’s <code>I/O</code> operation concludes, it will fall in line behind any other threads that are waiting to acquire the <code>GIL</code>. Seems like the ideal solution to our problems. WRONG.</p><p>Hopping threads is a kernel level operation. This means the OS has to context-switch between <code>user-mode</code> and <code>kernel-mode</code>. The kernel then has to save the context of the old thread and load the entire context of the new thread before making the switch back to user-mode. This is an accurate, safe but slow operation.</p><p>So while the <code>gthread</code> worker type ensures that a worker is not entirely handicapped while doing <code>I/O</code>, it comes at the cost of having to constantly switch thread between threads, which is not a cheap operation and relies heavily on the kernel and the os scheduler.</p><p>Note that the <code>GIL</code> allows only one thread within a process to execute python bytecode at once, so true multithreading (parallelism) for CPU-intensive tasks isn’t possible in python.</p><h2 id=cooperative-multitasking>Cooperative multitasking<a href=#cooperative-multitasking class=hanchor arialabel=Anchor>&#8983;</a></h2><p>This is where the <code>gunicorn</code> “<code>gevent</code>” worker type comes in.
<code>gevent</code> “greenlets” are lightweight python objects that can be spawned 100s and thousands of times within a process.</p><p>Greenlets work on the principle of “cooperative multitasking”. Which means, a greenlet will continue to work on a request, without interruption, until it hits a blocking <code>I/O</code> call. There’s no external body like the os scheduler or kernel making decisions about when control should be confiscated from one thread and transferred to another.</p><p>The OS has absolutely no idea that greenlets exist. A “context-swtich” between greenlets is just a function call within the same process. There’s no transition to kernel mode needed to make this happen.</p><p>Then why would one bother using threads at all?
While greenlets are incredibly lightweight and dramatically boost your worker’s throughput, there’re no guard rails preventing a greenlet from completely monopolizing the thread it&rsquo;s running on, and pushing every other greenlet into starvation.</p><p>If a thread enters an infinite CPU-bound loop, the OS can forcefully preempt it and allow other threads to run.</p><p>There’s no mechanism that enables <code>gevent</code> to do the same.</p><p>It relies on the greenlet to voluntarily yield control after hitting a blocking <code>I/O</code> operation, leaving the other greenlets vulnerable to starvation, should the current greenlet hit an infinite CPU bound loop.</p><h3 id=what-does-gevent-under-the-hood>What does Gevent under the hood<a href=#what-does-gevent-under-the-hood class=hanchor arialabel=Anchor>&#8983;</a></h3><p>But how does synchronous code (<code>requests.get()</code>, <code>time.sleep()</code>, etc.) suddenly start behaving cooperatively without you changing a single line? The answer is this goofy technique called monkey-patching.</p><p>The default blocking implementations of modules like <code>socket</code> and <code>requests</code>, get modified by <code>gevent</code> to use its cooperative methods at runtime.</p><p>Emphasis on “modified” because the entire module doesn&rsquo;t actually get replaced, just the blocking functions and classes.</p><p>When you do
<code>from gevent import monkey</code><br><code>monkey.patch_all()</code></p><p>At the beginning of your <code>wsgi.py</code> file, <code>patch_all()</code> essentially imports all blocking versions of all compatible modules, and modifies them by replacing their blocking implementation with its cooperative ones.
This is a one-time operation that happens before any of your other application code executes. So everytime you import the <code>requests</code> or <code>socket</code> module in your source code, you’ll actually be running the version that has been patched by <code>gevent</code>.</p><p><strong>Tldr:</strong></p><p>In your <code>wsgi.py</code> file, call the <code>patch_all()</code> method before any of the boilerplate code that you already have. You <code>wsgi.py</code> file should look something like this</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> gevent <span style=color:#f92672>import</span> monkey
</span></span><span style=display:flex><span>monkey<span style=color:#f92672>.</span>patch_all()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> django.core.wsgi <span style=color:#f92672>import</span> get_wsgi_application
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ<span style=color:#f92672>.</span>setdefault(<span style=color:#e6db74>&#39;DJANGO_SETTINGS_MODULE&#39;</span>, <span style=color:#e6db74>&#39;myproject.settings&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>application <span style=color:#f92672>=</span> get_wsgi_application()
</span></span></code></pre></div><p>While deploying your application (im assuming you’re already using a production ready WSGI server)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gunicorn myproject.wsgi:application <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --worker-class gevent <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --workers <span style=color:#ae81ff>4</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --bind 0.0.0.0:8000
</span></span></code></pre></div><p>In the next part of this series, I’ll do a benchmark that compares the throughput of a single threaded gunicorn worker vs a gevent one.</p></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2025 Powered by <a href=https://gohugo.io>Hugo</a></span>
<span>:: <a href=https://github.com/mirus-ua/hugo-theme-re-terminal target=_blank>Theme</a> made by <a href=https://github.com/mirus-ua target=_blank>Mirus</a></span></div></div></footer><script type=text/javascript src=../../bundle.min.js></script></div></body></html>